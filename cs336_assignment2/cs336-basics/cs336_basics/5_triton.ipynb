{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0b8e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1285458047.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'triton'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b02ce0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'triton' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-292442957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriton\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'triton' is not defined"
     ]
    }
   ],
   "source": [
    "print(triton.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9368a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def weighted_sum_fwd(\n",
    "    x_ptr, weight_ptr,\n",
    "    output_ptr,\n",
    "    x_stride_row, x_stride_dim,\n",
    "    weight_stride_dim,\n",
    "    output_stride_row,\n",
    "    ROWS,D,\n",
    "    ROWS_TILE_SIZE:tl.constexpr, D_TILE_SIZE:tl.constexpr,#声明编译时常量，Tile分块的形状在编译时必须为已知\n",
    "    ):\n",
    "    row_tile_idx = tl.program_id(0)#检查正在运行哪个thread block，即获取当前线程块处理的张量子块\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "        x_ptr,\n",
    "        shape=(ROWS, D,),\n",
    "        strides = (x_stride_row, x_stride_dim),\n",
    "        offsets = (row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape = (ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1,0),#列优先的存储顺序\n",
    "    )\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,),\n",
    "        strides = (weight_stride_dim,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),#行优先\n",
    "    )\n",
    "    output_block_ptr=tl.make_bolcj_ptr(\n",
    "        output_ptr,\n",
    "        shape=(ROWS,),\n",
    "        strides = (output_stride_row,),\n",
    "        offsets = (row_tile_idx*ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    #initialize a buffer to write to\n",
    "    output = tl.zeros((ROWS_TILE_SIZE,), dtype=tl.float32)\n",
    "\n",
    "    for i in range(tl.cdiv(D,D_TILE_SIZE)):#向上取整的除法\n",
    "        #load the current block pointer\n",
    "        #考虑行、列无法整除块，需要对2个维度进行边界检查\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0,1), padding_option = 'zero')#从指向的内存位置加载数据，只对第二个维度进行边界检查，如果超出边界，填充为0\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "\n",
    "        #compute the weighted sum of the row\n",
    "        output += tl.sum(row*weight[None,:], axis = 1)\n",
    "\n",
    "        #移动指针到下一个块\n",
    "        x_block_ptr = x_block_ptr.advance(0, D_TILE_SIZE)\n",
    "        weight_block_ptr = weight_block_ptr.advance(ROWS_TILE_SIZE,)\n",
    "    \n",
    "    tl.store(output_block_ptr, output, boundary_check=(0,))\n",
    "\n",
    "\n",
    "#wrap the kernel in a pytorch autograd function   \n",
    "import torch, einops\n",
    "from einops import rearrange\n",
    "class WeightedSumFunc(torch.autofrad.Function): #torch.autofrad.Function是 PyTorch 中用于实现自定义前向和反向传播逻辑的基类。\n",
    "    @staticmethod \n",
    "    #静态方法不依赖类实例，可以直接通过类名调用。\n",
    "    def forward(ctx, x, weight):\n",
    "        D, output_dims = x.shape[-1, ], x.shape[:-1]\n",
    "\n",
    "        input_shape = x.shape\n",
    "        #输入二维化\n",
    "        x=rearrange(x, '... d -> (...) d') # '... d -> (...) d' 表示将所有前面的维度合并为一个维度，最后一维保持不变。\n",
    "\n",
    "        ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) ==1 and weight.shape[0]==D, 'Dimentsion mismatch'\n",
    "        assert x.is_cuda and weight.is_cuda, 'Expected CUDA tensors'\n",
    "        assert x.is_contiguous(), 'Our pointer arithmetic will assume contiguous x'\n",
    "\n",
    "        ctx.D_TILE_SIZE = triton.next_power_of_2(D)//16 #将 D 向上取整到最近的 2 的幂，再将结果除以16，表示每个线程块处理的列数\n",
    "        ctx.ROWS_TILE_SIZE=16 # 每个线程同时处理16行数据\n",
    "        ctx.input_shape = input_shape\n",
    "        \n",
    "        #初始化一个空的结果张量，但元素不一定为0\n",
    "        y = torch.empty(output_dims, decice = x.device)\n",
    "\n",
    "        n_rows = y.numel()#输出y的元素总数\n",
    "\n",
    "        #weight_sum_fwd函数是已经被@triton.jit装饰的函数体，因此在Triton中，可以使用[]，即用于指定内核的网格大小（grid size），即 GPU 上线程块的分布方式。\n",
    "        #triton的内核函数调用语法如下：kernel[grid](args)\n",
    "        #这里的grid是一个元组\n",
    "        weighted_sum_fwd[(cdiv(n_rows, ctx.ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),#输出维度的步长\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE = ctx.ROWS_TILE_SIZE, D_TILE_SIZE = ctx.D_TILE_SIZE, \n",
    "        )\n",
    "        return y.view(input_shape[:-1])\n",
    "\n",
    "@triton.jit\n",
    "def weighted_sum_backward(\n",
    "    x_ptr, weight_ptr, # input\n",
    "    grad_output_ptr, # grad input\n",
    "    grad_x_ptr, partial_grad_weight_ptr, # grad outputs\n",
    "    stride_xr, stride_xd,\n",
    "    stride_wd,\n",
    "    stride_gr,\n",
    "    stride_gxr, stride_gxd,\n",
    "    stride_gwb, stride_gwd,\n",
    "    NUM_ROWS, D,\n",
    "    ROWS_TILE_SIZE:tl.constexpr, D_TILE_SIZE:tl.constexpr,\n",
    "):\n",
    "    row_title_idx = tl.program_id(0)\n",
    "    n_row_tiles = tl.num_programs(0)\n",
    "\n",
    "    #inputs\n",
    "    grad_outputs_block_ptr = tl.make_block_ptr(\n",
    "        grad_output_ptr,\n",
    "        shape = (NUM_ROWS,), strides = (stride_gr,),\n",
    "        offsets = (row_title_idx * ROWS_TILE_SIZE,),\n",
    "        block_shape=(ROWS_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    x_block_ptr = tl.make_block_ptr(\n",
    "    x_ptr,\n",
    "    shape=(NUM_ROWS, D,), strides=(stride_xr, stride_xd),\n",
    "    offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "    block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "    order=(1, 0),\n",
    "    )\n",
    "\n",
    "    weight_block_ptr = tl.make_block_ptr(\n",
    "        weight_ptr,\n",
    "        shape=(D,), strides=(stride_wd,),\n",
    "        offsets=(0,),\n",
    "        block_shape=(D_TILE_SIZE,),\n",
    "        order=(0,),\n",
    "    )\n",
    "\n",
    "    grad_x_block_ptr = tl.make_block_ptr(\n",
    "        grad_x_ptr,\n",
    "        shape=(NUM_ROWS, D,), strides=(stride_gxr, stride_gxd),\n",
    "        offsets=(row_tile_idx * ROWS_TILE_SIZE, 0),\n",
    "        block_shape=(ROWS_TILE_SIZE, D_TILE_SIZE),\n",
    "        order=(1, 0),\n",
    "        )\n",
    "\n",
    "    partial_grad_weight_block_ptr = tl.make_block_ptr(\n",
    "        partial_grad_weight_ptr,\n",
    "        shape=(n_row_tiles, D,), strides=(stride_gwb,stride_gwd),\n",
    "        offsets=(row_tile_idx, 0),\n",
    "        block_shape=(1, D_TILE_SIZE),\n",
    "        order=(1,0),\n",
    "    )\n",
    "\n",
    "    for i in range(tl.cdiv(D,D_TILE_SIZE)):\n",
    "        grad_output = tl.load(grad_outputs_block_ptr, boundary_check=(0,), padding_option='zero')#加载输出梯度\n",
    "\n",
    "        #outer product for grad_x\n",
    "        weight = tl.load(weight_block_ptr, boundary_check=(0,), padding_option='zero')\n",
    "        grad_x_row = grad_output[:, None] * weight[None, :] #输出梯度与w的外积\n",
    "        tl.store(grad_x_block_ptr, grad_x_row, boundary_check=(0,1))\n",
    "\n",
    "        #权重梯度\n",
    "        row = tl.load(x_block_ptr, boundary_check=(0,1), padding_option='zero')\n",
    "        grad_weight_row = tl.sum(row*grad_output[:,None], axis=0, keep_dims=True)\n",
    "        tl.store(partial_grad_weight_block_ptr, grad_weight_row, boundary_check=(1,))\n",
    "\n",
    "        #移走下一个块的指针\n",
    "        x_block_ptr = x_block_ptr.advance((0, D_TILE_SIZE))\n",
    "        weight_block_ptr = weight_block_ptr.advance((D_TILE_SIZE,))\n",
    "        partial_grad_weight_block_ptr = partial_grad_weight_block_ptr.advance((0,D_TILE_SIZE))\n",
    "        grad_x_block_ptr = grad_x_block_ptr.advance((0, D_TILE_SIZE))\n",
    "\n",
    "\n",
    "class WeightedSumFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, weight):\n",
    "        D, output_dims = x.shape[-1, ], x.shape[:-1]\n",
    "        input_shape = x.shape\n",
    "        #输入二维化\n",
    "        x=rearrange(x, '... d -> (...) d') # '... d -> (...) d' 表示将所有前面的维度合并为一个维度，最后一维保持不变。\n",
    "\n",
    "        ctx.save_for_backward(x, weight)\n",
    "\n",
    "        assert len(weight.shape) ==1 and weight.shape[0]==D, 'Dimentsion mismatch'\n",
    "        assert x.is_cuda and weight.is_cuda, 'Expected CUDA tensors'\n",
    "        assert x.is_contiguous(), 'Our pointer arithmetic will assume contiguous x'\n",
    "\n",
    "        ctx.D_TILE_SIZE = triton.next_power_of_2(D)//16 #将 D 向上取整到最近的 2 的幂，再将结果除以16，表示每个线程块处理的列数\n",
    "        ctx.ROWS_TILE_SIZE=16 # 每个线程同时处理16行数据\n",
    "        ctx.input_shape = input_shape\n",
    "        \n",
    "        #初始化一个空的结果张量，但元素不一定为0\n",
    "        y = torch.empty(output_dims, device = x.device)\n",
    "\n",
    "        n_rows = y.numel()#输出y的元素总数\n",
    "        #weight_sum_fwd函数是已经被@triton.jit装饰的函数体，因此在Triton中，可以使用[]，即用于指定内核的网格大小（grid size），即 GPU 上线程块的分布方式。\n",
    "        #triton的内核函数调用语法如下：kernel[grid](args)\n",
    "        #这里的grid是一个元组\n",
    "\n",
    "        weighted_sum_fwd[(cdiv(n_rows, ctx.ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            y,\n",
    "            x.stride(0), x.stride(1),#输出维度的步长\n",
    "            weight.stride(0),\n",
    "            y.stride(0),\n",
    "            ROWS=n_rows, D=D,\n",
    "            ROWS_TILE_SIZE = ctx.ROWS_TILE_SIZE, D_TILE_SIZE = ctx.D_TILE_SIZE, \n",
    "        )\n",
    "        return y.view(input_shape[:-1])\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        x, weight = ctx.saved_tensors\n",
    "        ROWS_TILE_SIZE, D_TILE_SIZE,= ctx.ROWS_TILE_SIZE, ctx.D_TILE_SIZE,\n",
    "        n_rows, D = x.shape #.shape返回元组，可以与解包的特性配合使用\n",
    "\n",
    "        partial_grad_weight = torch.empty((cdiv(n_rows, ROWS_TILE_SIZE), D), device = x.device, dtype=x.dtype)\n",
    "        grad_x = torch.empty_like(x)\n",
    "        #torch.empty_like用于创建张量的核心函数，核心作用是生成一个和指定张量「形状、数据类型、设备、布局完全相同」但未初始化的空张量——“未初始化” 意味着张量内的值是内存中的随机垃圾值，不会自动填充 0 或其他默认值\n",
    "        #torch.empty()是手动指定，而不是模仿已有的张量创建未初始化张量\n",
    "        weighted_sum_backward[(cdiv(n_rows, ROWS_TILE_SIZE),)](\n",
    "            x, weight,\n",
    "            grad_out,\n",
    "            grad_x, partial_grad_weight,\n",
    "            x.stride(0), x.stride(1),\n",
    "            weight.stride(0),\n",
    "            grad_out.stride(0),\n",
    "            grad_x.stride(0),grad_x.stride(1),\n",
    "            partial_grad_weight.stride(0), partial_grad_weight.stride(1),\n",
    "            NUM_ROWS = n_rows, D=D,\n",
    "            ROWS_TILE_SIZE=ROWS_TILE_SIZE, D_TILE_SIZE=D_TILE_SIZE,\n",
    "        )\n",
    "        grad_weight = partial_grad_weight.sum(axis=0)\n",
    "        return grad_x, grad_weight\n",
    "    \n",
    "\n",
    "if '__name__' == 'main':\n",
    "    x = torch.randn(3,2,device ='cuda')\n",
    "    weight = torch.randn(2,3,device ='cuda')\n",
    "    f_weightedsum = WeightedSumFunc.apply # torch.autograd.Function 的一个方法，用于调用自定义的前向和反向传播逻辑。\n",
    "    result = f_weightedsum(x, weight) #ctx因为是autograd.Function的入口封装方法打来的，因此会自动创建一个ctx（上下文对象），因此只需要关注ctx后面的参数\n",
    "    #forward 和 backward 就是通过固定的函数名称（关键字） 被 PyTorch 识别的 ——PyTorch 内部会严格检查继承自 autograd.Function 的类是否实现了名为 forward 和 backward 的静态方法，\n",
    "    # 这是 PyTorch 自动微分机制的 “约定式编程” 规则\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe8f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa706ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4d41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be52390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6dde75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e3442b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
