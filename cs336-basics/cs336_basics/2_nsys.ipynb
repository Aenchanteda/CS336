{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d16f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.cuda.nvtx as nvtx\n",
    "import torch\n",
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 200\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "d_model = 512\n",
    "\n",
    "@nvtx.range('scaled dot product attention')\n",
    "def annotated_scaled_dot_product_attention(Q, K, V, mask, seq_len, batch_dims):\n",
    "    # Create Q, K, V tensors (single head attention)\n",
    "    Q = torch.randn(batch_size, seq_len, d_k, device='cuda')\n",
    "    K = torch.randn(batch_size, seq_len, d_k, device='cuda')\n",
    "    V = torch.randn(batch_size, seq_len, d_v, device='cuda')\n",
    "    o_proj_weight = torch.randn(d_model, d_v, device='cuda')\n",
    "\n",
    "    batch_size_actual = Q.shape[0]\n",
    "    d_k = Q.shape[-1]\n",
    "   \n",
    "    # 创建因果掩码：每个位置只能看到之前的token\n",
    "    # causal_mask: (seq_len, seq_len)，位置(i,j)当i>=j时为True\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device='cuda'))\n",
    "    # 扩展到批次和头的维度：(1, ..., 1, seq_len, seq_len)\n",
    "    # 需要 len(batch_dims) + 1 个1（batch_dims + num_heads维度）\n",
    "    mask = mask.expand(batch_size_actual, seq_len, seq_len)\n",
    "\n",
    "    with nvtx.range('computing attention scores'):\n",
    "        # 开始运算attention\n",
    "        # head_i = Attention(Q_i, K_i, V_i)，使用 scaled dot-product attention\n",
    "        scores = Q @ K.transpose(-2, -1)  # (batch_size, seq_len, seq_len)\n",
    "        scores = scores / math.sqrt(d_k)  # 缩放\n",
    "        # 应用因果掩码：将不允许的位置设为负无穷\n",
    "        scores = scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "    with nvtx.range('computing softmax'):\n",
    "        attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "        attention = attention_weights @ V  # (batch_size, seq_len, d_v)\n",
    "\n",
    "    with nvtx.range('final matmul'):\n",
    "        output = attention @ o_proj_weight.T\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ab4e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2729731130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mtriton_kernel_flash_attention_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "@triton.jit\n",
    "def flash_fwd_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr,\n",
    "    O_ptr, L_ptr,\n",
    "    stride_qb, stride_qq, stride_qd,# stride_qb是Q在batch维度上的stride，标识Q从一个batch跳到下一个batch需要经过的元素数量\n",
    "    stride_kb, stride_kk, stride_kd,\n",
    "    stride_vb, stride_vk, stride_vd,\n",
    "    stride_ob, stride_oq, stride_od,\n",
    "    stride_lb, stride_lq,\n",
    "    N_QUERIES, N_KEYS,\n",
    "    scale,\n",
    "    D:tl.constexpr,\n",
    "    Q_TILE_SIZE:tl.constexpr,\n",
    "    K_TILE_SIZE:tl.constexpr,\n",
    "):\n",
    "    #grid的形状是[Q_TILE_SIZE, batch_size]\n",
    "    query_tile_index = tl.program_id(0) #当前program在第0维的索引，标识当前program处理的数据范围\n",
    "    batch_index = tl.program_id(1)\n",
    "\n",
    "    Q_block_ptr = tl.make_block_ptr(\n",
    "        Q_ptr + batch_index*stride_qb,#处理不同batch下的Q数据，Q.shape=[4,128,64],batch0在地质Q_ptr+0, batch1在地址Q_ptr+8192,以此类推。\n",
    "        shape = (N_QUERIES, D), #tensor的完整形状（全局）\n",
    "        strides = (stride_qq, stride_qd),\n",
    "        offsets = (query_tile_index * Q_TILE_SIZE, 0), # tile在tensor中的起始位置\n",
    "        block_shape = (Q_TILE_SIZE, D), #每次加载的tile大小（局部）\n",
    "        order = (1,0), #储存顺序为列优先\n",
    "    )\n",
    "\n",
    "    K_block_ptr = tl.make_block_ptr(\n",
    "        K_ptr + batch_index * stride_kb,\n",
    "        shape = (N_KEYS, D),\n",
    "        strides = (stride_kk, stride_kd),\n",
    "        offsets = (0, 0),\n",
    "        block_shape = (K_TILE_SIZE, D),\n",
    "        order = (1,0), #储存顺序为列优先\n",
    "    )\n",
    "\n",
    "    V_block_ptr = tl.make_block_ptr(\n",
    "        V_ptr + batch_index * stride_vb,\n",
    "        shape = (N_KEYS, D),\n",
    "        strides = (stride_vk, stride_vd),\n",
    "        offsets = (0, 0),\n",
    "        block_shape = (K_TILE_SIZE, D),\n",
    "        order = (1,0), #储存顺序为列优先\n",
    "    )\n",
    "\n",
    "    O_block_ptr = tl.make_block_ptr(\n",
    "        O_ptr + batch_index * stride_ob,\n",
    "        shape = (N_QUERIES,D),\n",
    "        strides = (stride_oq, stride_od),\n",
    "        offsets = (0, 0),\n",
    "        block_shape = (Q_TILE_SIZE, D),\n",
    "        order = (1,0), #储存顺序为列优先\n",
    "    )\n",
    "\n",
    "    L_block_ptr = tl.make_block_ptr(\n",
    "        L_ptr + batch_index * stride_lb,\n",
    "        shape = (N_QUERIES, ),\n",
    "        strides = (stride_lb, stride_lq),\n",
    "        offsets = (0, 0),\n",
    "        block_shape = (Q_TILE_SIZE, ),\n",
    "        order = (1,0), #储存顺序为列优先\n",
    "    )\n",
    "\n",
    "    Q = tl.load(Q_block_ptr, boundary_check=(0,1), padding_option = 'zero')\n",
    "    K = tl.load(K_block_ptr, boundary_check = (0,1), padding_option = 'zero')\n",
    "    V = tl.load(V_block_ptr, boundary_check = (0,1), padding_option = 'zero')\n",
    "    O = tl.load(O_block_ptr, boundary_check = (0,1), padding_option = 'zero')\n",
    "    L = tl.load(L_block_ptr, boundary_check = (0,1), padding_option = 'zero')\n",
    "    for i in range(0, N_QUERIES, Q_TILE_SIZE):\n",
    "        q_end = min(i+ Q_TILE_SIZE, N_QUERIES)\n",
    "        q_tile = Q[..., i:q_end, :]\n",
    "        o = tl.zeros(q_tile, dtype = q_tile.dtype)\n",
    "        l = tl.zeros(*q_tile.shape[:-1],dtype = q_tile.dtype)\n",
    "        m = tl.full((*q_tile.shape[:-1],), float('-inf'), dtype = q_tile.dtype)\n",
    "        m.fill_(float('-inf'))\n",
    "        for j in range(0, N_KEYS, K_TILE_SIZE):\n",
    "            k_end = min(j+K_TILE_SIZE, N_KEYS)\n",
    "            k_tile = K[...,j:k_end, :]\n",
    "            v_tile = V[...,j:k_end, :]\n",
    "            S = tl.dot(q_tile @ k_tile.transpose(-2,-1)) / scale\n",
    "            m_new = torch.maximum(m, S.max(dim=-1)[0])\n",
    "            P = tl.exp(S - m_new.unsqueeze(-1), dtype = v_tile.dtype)\n",
    "            l = tl.exp(m - m_new)*l + P.sum(dim=-1)\n",
    "            o = tl.exp(m.unsqueeze(-1)-m_new.unsqueeze(-1)) * o + tl.dot(P, v_tile)\n",
    "            m = m_new\n",
    "            K_block_ptr = K_block_ptr.advance(K_TILE_SIZE,0)\n",
    "            V_block_ptr = V_block_ptr.advance(K_TILE_SIZE,0)\n",
    "        o = o / l.unsqueeze(-1)\n",
    "        l = m + tl.log(l)\n",
    "        O[...,i:q_end, :] = o\n",
    "        L[...,i:q_end] = l\n",
    "        Q_block_ptr = Q_block_ptr.advance(Q_TILE_SIZE,0)\n",
    "        O_block_ptr = V_block_ptr.advance(Q_TILE_SIZE,0)\n",
    "        L_block_ptr = V_block_ptr.advance(Q_TILE_SIZE,0)\n",
    "    tl.store(O, boundary_check = (0,))\n",
    "\n",
    "\n",
    "#uv run pytest -k test_flash_forward_pass_triton\n",
    "class triton_kernel_flash_attention_fwd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V, is_causal = False):\n",
    "        batch_size, N_QUERIES, D = Q.shape\n",
    "        scale  = math.sqrt(D)\n",
    "        N_KEYS = K.shape[-2] # 每个线程同时处理16行数据\n",
    "\n",
    "        Q_TILE_SIZE = Q.shape[-2]\n",
    "        K_TILE_SIZE = K.shape[-2]\n",
    "        O = torch.zeros(Q)\n",
    "        L = torch.zeros(*Q.shape[:-1])\n",
    "        ctx.save_for_backward(O, L)\n",
    "        flash_fwd_kernel[Q_TILE_SIZE, batch_size](\n",
    "            Q, K, V,\n",
    "            O, L,\n",
    "\n",
    "            Q.stride(0), \n",
    "            Q.stride(1), \n",
    "            Q.stride(2), \n",
    "\n",
    "            K.stride(0), \n",
    "            K.stride(1), \n",
    "            K.stride(2), \n",
    "\n",
    "            V.stride(0), \n",
    "            V.stride(1), \n",
    "            V.stride(2), \n",
    "\n",
    "            O.stride(0), \n",
    "            O.stride(1), \n",
    "            O.stride(2), \n",
    "\n",
    "            L.stride(0), \n",
    "            L.stride(1), \n",
    "        \n",
    "            N_QUERIES, \n",
    "            N_KEYS,\n",
    "            scale,\n",
    "            D,\n",
    "            Q_TILE_SIZE,\n",
    "            K_TILE_SIZE,\n",
    "        )\n",
    "\n",
    "        ctx.save_for_backward (Q, K, V, O, L)\n",
    "        ctx.is_causal = is_causal\n",
    "        return O\n",
    "    @staticmethod\n",
    "    def backward(ctx):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92812d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be661753",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191a4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NEW IMPLEMENTATION: Using cProfile for performance analysis\n",
    "# ============================================================================\n",
    "# This version works on both CPU and GPU, doesn't require NVIDIA tools\n",
    "# ============================================================================\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "from pstats import SortKey\n",
    "import torch\n",
    "import math\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 200\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "d_model = 512\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None, seq_len=200, batch_dims=1):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention (single head)\n",
    "    Using cProfile for performance analysis instead of nvtx\n",
    "    \"\"\"\n",
    "    # Use provided Q, K, V or create defaults\n",
    "    if Q is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        Q = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "    if K is None:\n",
    "        device = Q.device if Q is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        K = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "    if V is None:\n",
    "        device = Q.device if Q is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        V = torch.randn(batch_size, seq_len, d_v, device=device)\n",
    "    \n",
    "    # Initialize output projection weight\n",
    "    device = Q.device\n",
    "    o_proj_weight = torch.randn(d_model, d_v, device=device)\n",
    "    \n",
    "    # Get dimensions\n",
    "    batch_size_actual = Q.shape[0]\n",
    "    d_k = Q.shape[-1]\n",
    "    # 从权重字典中提取可训练参数\n",
    "    optimizer = AdamW(\n",
    "        params,\n",
    "        lr,\n",
    "        betas,\n",
    "        eps,\n",
    "        weight_decay\n",
    "    )\n",
    "    # Create causal mask if not provided\n",
    "    if mask is None:\n",
    "        # 创建因果掩码：每个位置只能看到之前的token\n",
    "        # causal_mask: (seq_len, seq_len)，位置(i,j)当i>=j时为True\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device))\n",
    "        # 扩展到批次维度：(batch_size, seq_len, seq_len)\n",
    "        mask = causal_mask.expand(batch_size_actual, seq_len, seq_len)\n",
    "\n",
    "    # Computing attention scores (equivalent to nvtx.range('computing attention scores'))\n",
    "    scores = Q @ K.transpose(-2, -1)  # (batch_size, seq_len, seq_len)\n",
    "    scores = scores / math.sqrt(d_k)  # Scale\n",
    "    # Apply causal mask\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "    # Computing softmax (equivalent to nvtx.range('computing softmax'))\n",
    "    attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "    attention = attention_weights @ V  # (batch_size, seq_len, d_v)\n",
    "\n",
    "    # Final matmul (equivalent to nvtx.range('final matmul'))\n",
    "    output = attention @ o_proj_weight.T  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Profile wrapper function\n",
    "def profile_attention_function(n_iterations=100):\n",
    "    \"\"\"Profile the attention function using cProfile\"\"\"\n",
    "    # Create test inputs\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    Q = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "    K = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "    V = torch.randn(batch_size, seq_len, d_v, device=device)\n",
    "    \n",
    "    # Create profiler\n",
    "    profiler = cProfile.Profile()\n",
    "    \n",
    "    # Profile the function\n",
    "    profiler.enable()\n",
    "    for _ in range(n_iterations):\n",
    "        output = scaled_dot_product_attention(Q, K, V, None, seq_len, batch_dims=1)\n",
    "    profiler.disable()\n",
    "    \n",
    "    # Create a string buffer to capture stats\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(profiler, stream=s)\n",
    "    ps.sort_stats(SortKey.CUMULATIVE)  # Sort by cumulative time\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"=\"*80)\n",
    "    print(\"cProfile Results - Top 20 functions by cumulative time\")\n",
    "    print(\"=\"*80)\n",
    "    ps.print_stats(20)\n",
    "    print(s.getvalue())\n",
    "    \n",
    "    # Save to file\n",
    "    profiler.dump_stats('attention_profile.prof')\n",
    "    print(\"\\n✓ Profile saved to 'attention_profile.prof'\")\n",
    "    print(\"  View with: python -m pstats attention_profile.prof\")\n",
    "    print(\"  Or install snakeviz: pip install snakeviz && snakeviz attention_profile.prof\")\n",
    "    \n",
    "    return output, profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ff54cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "cProfile Results - Top 20 functions by cumulative time\n",
      "================================================================================\n",
      "         901 function calls in 0.413 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      100    0.254    0.003    0.413    0.004 /var/folders/cp/vqdxm_290_j_myz50smbrjwc0000gn/T/ipykernel_7947/448703527.py:15(scaled_dot_product_attention)\n",
      "      100    0.053    0.001    0.053    0.001 {built-in method torch.softmax}\n",
      "      100    0.041    0.000    0.041    0.000 {method 'masked_fill' of 'torch._C.TensorBase' objects}\n",
      "      100    0.036    0.000    0.036    0.000 {built-in method torch.randn}\n",
      "      100    0.023    0.000    0.023    0.000 {built-in method torch.ones}\n",
      "      100    0.003    0.000    0.003    0.000 {built-in method torch.tril}\n",
      "      100    0.002    0.000    0.002    0.000 {method 'expand' of 'torch._C.TensorBase' objects}\n",
      "      100    0.001    0.000    0.001    0.000 {method 'transpose' of 'torch._C.TensorBase' objects}\n",
      "      100    0.000    0.000    0.000    0.000 {built-in method math.sqrt}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✓ Profile saved to 'attention_profile.prof'\n",
      "  View with: python -m pstats attention_profile.prof\n",
      "  Or install snakeviz: pip install snakeviz && snakeviz attention_profile.prof\n",
      "\n",
      "================================================================================\n",
      "Filtered results - Only torch functions:\n",
      "================================================================================\n",
      "         901 function calls in 0.413 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 10 to 7 due to restriction <'torch'>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      100    0.053    0.001    0.053    0.001 {built-in method torch.softmax}\n",
      "      100    0.041    0.000    0.041    0.000 {method 'masked_fill' of 'torch._C.TensorBase' objects}\n",
      "      100    0.036    0.000    0.036    0.000 {built-in method torch.randn}\n",
      "      100    0.023    0.000    0.023    0.000 {built-in method torch.ones}\n",
      "      100    0.003    0.000    0.003    0.000 {built-in method torch.tril}\n",
      "      100    0.002    0.000    0.002    0.000 {method 'expand' of 'torch._C.TensorBase' objects}\n",
      "      100    0.001    0.000    0.001    0.000 {method 'transpose' of 'torch._C.TensorBase' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x10776c040>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run profiling\n",
    "output, profiler = profile_attention_function(n_iterations=100)\n",
    "\n",
    "# The profiler object can be used for further analysis\n",
    "# For example, you can filter by specific functions:\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Filtered results - Only torch functions:\")\n",
    "print(\"=\"*80)\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.sort_stats('cumulative')\n",
    "stats.print_stats('torch', 10)  # Show top 10 torch-related functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20446cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "cProfile Results - Top 20 functions by cumulative time\n",
      "================================================================================\n",
      "         226 function calls (216 primitive calls) in 0.173 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 81 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.031    0.031    0.173    0.173 /var/folders/cp/vqdxm_290_j_myz50smbrjwc0000gn/T/ipykernel_15297/15839886.py:24(scaled_dot_product_attention)\n",
      "        1    0.000    0.000    0.093    0.093 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/_tensor.py:465(backward)\n",
      "        1    0.000    0.000    0.093    0.093 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/autograd/__init__.py:183(backward)\n",
      "        1    0.000    0.000    0.092    0.092 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/autograd/graph.py:764(_engine_run_backward)\n",
      "        1    0.092    0.092    0.092    0.092 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "        2    0.028    0.014    0.028    0.014 {built-in method torch.randn}\n",
      "        1    0.000    0.000    0.012    0.012 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/nn/functional.py:3014(cross_entropy)\n",
      "        1    0.012    0.012    0.012    0.012 {built-in method torch._C._nn.cross_entropy_loss}\n",
      "        1    0.000    0.000    0.007    0.007 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/optim/optimizer.py:464(wrapper)\n",
      "        1    0.005    0.005    0.007    0.007 /Users/richard/Documents/GitHub/cs336_assignment2/cs336-basics/cs336_basics/optimizer.py:50(step)\n",
      "        4    0.001    0.000    0.001    0.000 {built-in method torch.zeros_like}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method torch.softmax}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch.tril}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.square}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'masked_fill' of 'torch._C.TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch.sqrt}\n",
      "        1    0.000    0.000    0.000    0.000 /Users/richard/Documents/GitHub/cs336_assignment2/cs336-basics/cs336_basics/optimizer.py:31(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/optim/optimizer.py:339(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/_compile.py:21(inner)\n",
      "        2    0.000    0.000    0.000    0.000 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:596(_fn)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✓ Profile saved to 'attention_profile.prof'\n",
      "  View with: python -m pstats attention_profile.prof\n",
      "  Or install snakeviz: pip install snakeviz && snakeviz attention_profile.prof\n",
      "\n",
      "================================================================================\n",
      "Filtered results - Only torch functions:\n",
      "================================================================================\n",
      "         226 function calls (216 primitive calls) in 0.173 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 81 to 56 due to restriction <'torch'>\n",
      "   List reduced from 56 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.093    0.093 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/_tensor.py:465(backward)\n",
      "        1    0.000    0.000    0.093    0.093 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/autograd/__init__.py:183(backward)\n",
      "        1    0.000    0.000    0.092    0.092 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/autograd/graph.py:764(_engine_run_backward)\n",
      "        1    0.092    0.092    0.092    0.092 {method 'run_backward' of 'torch._C._EngineBase' objects}\n",
      "        2    0.028    0.014    0.028    0.014 {built-in method torch.randn}\n",
      "        1    0.000    0.000    0.012    0.012 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/nn/functional.py:3014(cross_entropy)\n",
      "        1    0.012    0.012    0.012    0.012 {built-in method torch._C._nn.cross_entropy_loss}\n",
      "        1    0.000    0.000    0.007    0.007 /Users/richard/opt/anaconda3/envs/deepDR/OneRel_chinese/lib/python3.8/site-packages/torch/optim/optimizer.py:464(wrapper)\n",
      "        4    0.001    0.000    0.001    0.000 {built-in method torch.zeros_like}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method torch.softmax}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x105d477c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# NEW IMPLEMENTATION: Using cProfile for performance analysis\n",
    "# ============================================================================\n",
    "# This version works on both CPU and GPU, doesn't require NVIDIA tools\n",
    "# ============================================================================\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "from pstats import SortKey\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 200\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "d_model = 512\n",
    "from optimizer import AdamW\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None, seq_len=200, batch_dims=1):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention (single head)\n",
    "    Using cProfile for performance analysis instead of nvtx\n",
    "    \"\"\"\n",
    "    # Use provided Q, K, V or create defaults\n",
    "    if Q is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        Q = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "    if K is None:\n",
    "        device = Q.device if Q is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        K = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "    if V is None:\n",
    "        device = Q.device if Q is not None else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        V = torch.randn(batch_size, seq_len, d_v, device=device)\n",
    "    \n",
    "    # Initialize output projection weight\n",
    "    vocab_size = 5054\n",
    "    device = Q.device\n",
    "    o_proj_weight = torch.nn.Parameter(torch.randn(d_model, d_v, device=device))\n",
    "    lm_head_weight = torch.nn.Parameter(torch.randn(vocab_size, d_model, device = device))\n",
    "\n",
    "    # Get dimensions\n",
    "    batch_size_actual = Q.shape[0]\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "\n",
    "    # 从权重字典中提取可训练参数\n",
    "    optimizer = AdamW(\n",
    "        params=[o_proj_weight, lm_head_weight]\n",
    "    )\n",
    "    optimizer.zero_grad()\n",
    "    # Create causal mask if not provided\n",
    "    if mask is None:\n",
    "        # 创建因果掩码：每个位置只能看到之前的token\n",
    "        # causal_mask: (seq_len, seq_len)，位置(i,j)当i>=j时为True\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=device))\n",
    "        # 扩展到批次维度：(batch_size, seq_len, seq_len)\n",
    "        mask = causal_mask.expand(batch_size_actual, seq_len, seq_len)\n",
    "\n",
    "    # Computing attention scores (equivalent to nvtx.range('computing attention scores'))\n",
    "    scores = Q @ K.transpose(-2, -1)  # (batch_size, seq_len, seq_len)\n",
    "    scores = scores / math.sqrt(d_k)  # Scale\n",
    "    # Apply causal mask\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "    # Computing softmax (equivalent to nvtx.range('computing softmax'))\n",
    "    attention_weights = torch.softmax(scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "    attention = attention_weights @ V  # (batch_size, seq_len, d_v)\n",
    "\n",
    "    # Final matmul (equivalent to nvtx.range('final matmul'))\n",
    "    output = attention @ o_proj_weight.T  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    #loss\n",
    "    logits = output @ lm_head_weight.T\n",
    "    targets = torch.randint(0, vocab_size, (batch_size_actual, seq_len), dtype = torch.long)\n",
    "    logits_flat = logits.view(-1, vocab_size)\n",
    "    targets_flat = targets.view(-1)\n",
    "    loss = cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# Profile wrapper function\n",
    "def profile_attention_function(n_iterations=100):\n",
    "    \"\"\"Profile the attention function using cProfile\"\"\"\n",
    "    # Create test inputs\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    Q = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "    K = torch.randn(batch_size, seq_len, d_k, device=device)\n",
    "    V = torch.randn(batch_size, seq_len, d_v, device=device)\n",
    "    \n",
    "    # Create profiler\n",
    "    profiler = cProfile.Profile()\n",
    "    _ = scaled_dot_product_attention(Q, K, V, None, seq_len, batch_dims=1)\n",
    "\n",
    "    # Profile the function\n",
    "    profiler.enable()\n",
    "    for _ in range(n_iterations):\n",
    "        output = scaled_dot_product_attention(Q, K, V, None, seq_len, batch_dims=1)\n",
    "    profiler.disable()\n",
    "    \n",
    "    # Create a string buffer to capture stats\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(profiler, stream=s)\n",
    "    ps.sort_stats(SortKey.CUMULATIVE)  # Sort by cumulative time\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"=\"*80)\n",
    "    print(\"cProfile Results - Top 20 functions by cumulative time\")\n",
    "    print(\"=\"*80)\n",
    "    ps.print_stats(20)\n",
    "    print(s.getvalue())\n",
    "    \n",
    "    # Save to file\n",
    "    profiler.dump_stats('attention_profile.prof')\n",
    "    print(\"\\n✓ Profile saved to 'attention_profile.prof'\")\n",
    "    print(\"  View with: python -m pstats attention_profile.prof\")\n",
    "    print(\"  Or install snakeviz: pip install snakeviz && snakeviz attention_profile.prof\")\n",
    "    \n",
    "    return output, profiler\n",
    "\n",
    "\n",
    "# Run profiling\n",
    "output, profiler = profile_attention_function(n_iterations=1)\n",
    "\n",
    "# The profiler object can be used for further analysis\n",
    "# For example, you can filter by specific functions:\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Filtered results - Only torch functions:\")\n",
    "print(\"=\"*80)\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.sort_stats('cumulative')\n",
    "stats.print_stats('torch', 10)  # Show top 10 torch-related functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b43b94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pra",
   "language": "python",
   "name": "torch_pra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
