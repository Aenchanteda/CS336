# 交叉熵（Cross-Entropy）详细说明

## 1. 基本概念

交叉熵是信息论中的一个重要概念，用于衡量两个概率分布之间的差异。在机器学习中，特别是在分类任务和语言模型中，交叉熵是最常用的损失函数之一。

### 1.1 信息论背景

- **熵（Entropy）**：衡量随机变量的不确定性
  $$H(X) = -\sum_{x} p(x) \log p(x)$$

- **交叉熵（Cross-Entropy）**：衡量使用分布 $q$ 来编码来自分布 $p$ 的样本所需的平均比特数
  $$H(p, q) = -\sum_{x} p(x) \log q(x)$$

- **KL散度（Kullback-Leibler Divergence）**：衡量两个分布的差异
  $$D_{KL}(p || q) = H(p, q) - H(p)$$

## 2. 在Transformer语言模型中的应用

### 2.1 损失函数定义

对于Transformer语言模型，给定训练集 $D$，交叉熵损失函数定义为：

$$\ell(\theta; D) = \frac{1}{|D|m} \sum_{x \in D} \sum_{i=1}^{m} - \log p_{\theta}(x_{i+1} | x_{1:i}) \quad (16)$$

**公式解析：**
- $\theta$：模型参数
- $D$：训练数据集
- $|D|$：训练集中序列的数量
- $m$：每个序列的长度
- $x_{1:i}$：序列中前 $i$ 个token
- $x_{i+1}$：第 $i+1$ 个token（要预测的目标）
- $p_{\theta}(x_{i+1} | x_{1:i})$：模型预测下一个token的概率

**归一化因子**：$\frac{1}{|D|m}$ 确保损失值不依赖于数据集大小和序列长度。

### 2.2 从Logits到概率

Transformer模型输出的是**未归一化的logits** $o_i \in \mathbb{R}^{\text{vocab\_size}}$，需要通过softmax函数转换为概率分布：

$$p(x_{i+1} | x_{1:i}) = \text{softmax}(o_i)[x_{i+1}] = \frac{\exp(o_i[x_{i+1}])}{\sum_{a=1}^{\text{vocab\_size}} \exp(o_i[a])} \quad (17)$$

**Softmax函数**：
- 将任意实数值的logits转换为概率分布
- 确保所有概率之和为1
- 指数运算使得较大的logits获得更高的概率

## 3. 数值稳定的实现

### 3.1 问题：数值溢出

直接计算交叉熵会遇到数值稳定性问题：

```python
# 不稳定的实现
logits = [100, 200, 300]  # 大数值
exp_logits = exp(logits)  # 可能溢出！
prob = exp_logits / sum(exp_logits)
loss = -log(prob[target])
```

### 3.2 解决方案：Log-Sum-Exp技巧

使用数学恒等式避免直接计算指数：

$$\text{CrossEntropy} = -\log \frac{\exp(o_i[x_{i+1}])}{\sum_{a} \exp(o_i[a])}$$

可以改写为：

$$\text{CrossEntropy} = -o_i[x_{i+1}] + \log \sum_{a} \exp(o_i[a])$$

进一步使用数值稳定形式（减去最大值）：

$$\text{CrossEntropy} = -o_i[x_{i+1}] + \max(o_i) + \log \sum_{a} \exp(o_i[a] - \max(o_i))$$

### 3.3 实现示例

```python
def cross_entropy_stable(logits, target):
    """
    数值稳定的交叉熵实现
    
    Args:
        logits: (batch_size, vocab_size) 未归一化的logits
        target: (batch_size,) 目标类别索引
    
    Returns:
        平均交叉熵损失（标量）
    """
    # 找到每个样本的最大logit值
    max_logits = torch.max(logits, dim=-1, keepdim=True)[0]
    
    # 减去最大值，避免溢出
    shifted_logits = logits - max_logits
    
    # 计算log-sum-exp
    log_sum_exp = torch.log(torch.sum(torch.exp(shifted_logits), dim=-1))
    
    # 提取目标类别的logit
    target_logits = logits.gather(1, target.unsqueeze(1)).squeeze(1)
    
    # 计算交叉熵：-target_logit + log_sum_exp
    # 注意：这里不需要再加max_logits，因为target_logit和log_sum_exp都减去了相同的max_logits
    loss = -target_logits + max_logits.squeeze() + log_sum_exp
    
    # 返回平均值
    return loss.mean()
```

## 4. 在代码库中的使用

### 4.1 函数签名

根据 `tests/adapters.py` 中的定义：

```python
def run_cross_entropy(
    inputs: Float[Tensor, "batch_size vocab_size"], 
    targets: Int[Tensor, "batch_size"]
) -> Float[Tensor, ""]:
    """
    计算平均交叉熵损失
    
    Args:
        inputs: inputs[i][j] 是第i个样本第j个类别的未归一化logit
        targets: 目标类别索引，每个值在 [0, vocab_size-1] 范围内
    
    Returns:
        所有样本的平均交叉熵损失（标量）
    """
```

### 4.2 测试用例

测试代码验证了：
1. **基本功能**：与PyTorch的`F.cross_entropy`结果一致
2. **数值稳定性**：即使logits值很大（如乘以1000），也能正确计算

```python
# 测试数值稳定性
large_inputs = 1000.0 * inputs
large_expected_cross_entropy = F.cross_entropy(
    large_inputs.view(-1, large_inputs.size(-1)), 
    targets.view(-1)
)
```

## 5. 为什么使用交叉熵？

### 5.1 优点

1. **概率解释**：直接优化模型输出的概率分布
2. **梯度特性**：提供良好的梯度信号，有利于训练
3. **信息论基础**：有坚实的数学理论基础
4. **广泛应用**：在分类和语言建模任务中表现优异

### 5.2 与其他损失函数的比较

- **均方误差（MSE）**：不适合分类任务，因为假设输出是连续值
- **绝对误差（MAE）**：梯度信号较弱
- **交叉熵**：专为分类设计，梯度信号强，训练效率高

## 6. 在语言模型中的具体流程

### 6.1 前向传播

1. **输入序列**：$x_1, x_2, \ldots, x_m$
2. **模型输出**：对每个位置 $i$，输出logits $o_i \in \mathbb{R}^{\text{vocab\_size}}$
3. **概率分布**：$p_{\theta}(x_{i+1} | x_{1:i}) = \text{softmax}(o_i)$
4. **损失计算**：对每个位置计算 $- \log p_{\theta}(x_{i+1} | x_{1:i})$

### 6.2 批量处理

在实际训练中，通常处理多个序列（batch）：

- **输入形状**：`(batch_size, seq_len, vocab_size)` 的logits
- **目标形状**：`(batch_size, seq_len)` 的token索引
- **损失计算**：先展平为 `(batch_size * seq_len, vocab_size)` 和 `(batch_size * seq_len,)`，然后计算平均损失

### 6.3 反向传播

交叉熵损失的梯度计算：

$$\frac{\partial \ell}{\partial o_i[j]} = \begin{cases}
p_{\theta}(j | x_{1:i}) - 1 & \text{if } j = x_{i+1} \\
p_{\theta}(j | x_{1:i}) & \text{otherwise}
\end{cases}$$

这个梯度形式非常优雅：
- 正确类别的梯度是负的（鼓励增加该类的logit）
- 其他类别的梯度是正的（鼓励减少这些类的logit）
- 梯度大小与预测概率成正比（预测越不确定，梯度越大）

## 7. 总结

交叉熵损失函数是Transformer语言模型训练的核心：

1. **理论基础**：基于信息论，衡量预测分布与真实分布的差异
2. **数学形式**：负对数似然，通过softmax将logits转换为概率
3. **数值稳定**：使用log-sum-exp技巧避免数值溢出
4. **训练效率**：提供良好的梯度信号，有利于模型优化
5. **广泛应用**：是语言模型和分类任务的标准损失函数

理解和正确实现交叉熵对于训练高质量的Transformer模型至关重要。

